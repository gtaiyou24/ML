{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実装の設計\n",
    "> *どのように実装すれば効率よくディープニューラルネットモデルを定義できるか*\n",
    "## 基本設計\n",
    "### TensorFlowによる実装\n",
    "<div style=\"text-align:center\">\n",
    "    <b>モデルの出力を定義</b><br>\n",
    "    ↓<br>\n",
    "    <b>誤差関数を定義</b><br>\n",
    "    ↓<br>\n",
    "    <b>モデルを学習</b>\n",
    "</div>\n",
    "TensorFlowでは、上記の流れをステップごとに関数化することを推奨している。\n",
    "\n",
    "> - `inference()` : モデル全体の設定を行い、モデルの出力・予測結果を返す\n",
    "> - `loss()` : モデルの誤差関数を定義し、誤差・損失を返す\n",
    "> - `training()` : モデルの学習を行い、学習結果(進み具合)を返す\n",
    "\n",
    "```python\n",
    "def inference(x):\n",
    "    \"\"\"\n",
    "    モデルの定義\n",
    "    \"\"\"\n",
    "    \n",
    "def loss(pred_y, y):\n",
    "    \"\"\"\n",
    "    誤差関数の定義\n",
    "    \n",
    "    Parameter\n",
    "    - pred_y : 予測値\n",
    "    - y : 正解データ\n",
    "    \"\"\"\n",
    "\n",
    "def training(loss):\n",
    "    \"\"\"\n",
    "    学習アルゴリズムの定義\n",
    "    \"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. データの準備\n",
    "    # 2. モデル設定\n",
    "    model = inference(x)\n",
    "    loss = loss(model, y)\n",
    "    train_step = training(loss)\n",
    "    # 3. モデル学習\n",
    "    # 4. モデル評価\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taiyou/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/taiyou/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/taiyou/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  validation loss: 2.1558669  validation accuracy: 0.26725\n",
      "epoch: 1  validation loss: 0.8131207  validation accuracy: 0.768\n",
      "epoch: 2  validation loss: 0.51363796  validation accuracy: 0.85125\n",
      "epoch: 3  validation loss: 0.40519613  validation accuracy: 0.88725\n",
      "epoch: 4  validation loss: 0.3408817  validation accuracy: 0.90225\n",
      "epoch: 5  validation loss: 0.30277404  validation accuracy: 0.914\n",
      "epoch: 6  validation loss: 0.26965433  validation accuracy: 0.92175\n",
      "epoch: 7  validation loss: 0.25186813  validation accuracy: 0.9305\n",
      "epoch: 8  validation loss: 0.23474042  validation accuracy: 0.93325\n",
      "epoch: 9  validation loss: 0.22533228  validation accuracy: 0.9335\n",
      "epoch: 10  validation loss: 0.21379852  validation accuracy: 0.93875\n",
      "epoch: 11  validation loss: 0.20155106  validation accuracy: 0.94375\n",
      "epoch: 12  validation loss: 0.20540118  validation accuracy: 0.94075\n",
      "epoch: 13  validation loss: 0.18442798  validation accuracy: 0.948\n",
      "epoch: 14  validation loss: 0.19221994  validation accuracy: 0.9455\n",
      "epoch: 15  validation loss: 0.18167248  validation accuracy: 0.95\n",
      "epoch: 16  validation loss: 0.17690219  validation accuracy: 0.95225\n",
      "epoch: 17  validation loss: 0.1770504  validation accuracy: 0.95075\n",
      "epoch: 18  validation loss: 0.16998078  validation accuracy: 0.9545\n",
      "epoch: 19  validation loss: 0.17046326  validation accuracy: 0.95225\n",
      "epoch: 20  validation loss: 0.16714188  validation accuracy: 0.95475\n",
      "epoch: 21  validation loss: 0.16665687  validation accuracy: 0.95425\n",
      "epoch: 22  validation loss: 0.16041708  validation accuracy: 0.95525\n",
      "epoch: 23  validation loss: 0.15855236  validation accuracy: 0.9585\n",
      "epoch: 24  validation loss: 0.15041552  validation accuracy: 0.9595\n",
      "epoch: 25  validation loss: 0.15221725  validation accuracy: 0.96025\n",
      "epoch: 26  validation loss: 0.15488507  validation accuracy: 0.96175\n",
      "epoch: 27  validation loss: 0.15251549  validation accuracy: 0.961\n",
      "epoch: 28  validation loss: 0.15054494  validation accuracy: 0.9585\n",
      "epoch: 29  validation loss: 0.15429148  validation accuracy: 0.96125\n",
      "epoch: 30  validation loss: 0.14869572  validation accuracy: 0.964\n",
      "epoch: 31  validation loss: 0.15499805  validation accuracy: 0.9595\n",
      "epoch: 32  validation loss: 0.15246592  validation accuracy: 0.96\n",
      "epoch: 33  validation loss: 0.15215927  validation accuracy: 0.96275\n",
      "epoch: 34  validation loss: 0.14504611  validation accuracy: 0.96275\n",
      "epoch: 35  validation loss: 0.14852378  validation accuracy: 0.96425\n",
      "epoch: 36  validation loss: 0.14871265  validation accuracy: 0.9635\n",
      "epoch: 37  validation loss: 0.14685602  validation accuracy: 0.96325\n",
      "epoch: 38  validation loss: 0.14678869  validation accuracy: 0.964\n",
      "epoch: 39  validation loss: 0.15447581  validation accuracy: 0.96425\n",
      "epoch: 40  validation loss: 0.15280874  validation accuracy: 0.96175\n",
      "epoch: 41  validation loss: 0.15573348  validation accuracy: 0.96\n",
      "epoch: 42  validation loss: 0.15451613  validation accuracy: 0.96175\n",
      "epoch: 43  validation loss: 0.15238962  validation accuracy: 0.9625\n",
      "epoch: 44  validation loss: 0.15102567  validation accuracy: 0.96325\n",
      "epoch: 45  validation loss: 0.14581442  validation accuracy: 0.96425\n",
      "epoch: 46  validation loss: 0.14803599  validation accuracy: 0.96525\n",
      "epoch: 47  validation loss: 0.14867923  validation accuracy: 0.96325\n",
      "epoch: 48  validation loss: 0.14484802  validation accuracy: 0.96575\n",
      "epoch: 49  validation loss: 0.15247767  validation accuracy: 0.96425\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8b845afeb88f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     })\n\u001b[1;32m    158\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "\n",
    "def inference(x, keep_prob, n_in, n_hiddens, n_out):\n",
    "    def weight_variable(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.zeros(shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    # 入力層 - 隠れ層、隠れ層 - 隠れ層\n",
    "    for i, n_hidden in enumerate(n_hiddens):\n",
    "        if i == 0:\n",
    "            input = x\n",
    "            input_dim = n_in\n",
    "        else:\n",
    "            input = output\n",
    "            input_dim = n_hiddens[i-1]\n",
    "\n",
    "        W = weight_variable([input_dim, n_hidden])\n",
    "        b = bias_variable([n_hidden])\n",
    "\n",
    "        h = tf.nn.relu(tf.matmul(input, W) + b)\n",
    "        output = tf.nn.dropout(h, keep_prob)\n",
    "\n",
    "    # 隠れ層 - 出力層\n",
    "    W_out = weight_variable([n_hiddens[-1], n_out])\n",
    "    b_out = bias_variable([n_out])\n",
    "    y = tf.nn.softmax(tf.matmul(output, W_out) + b_out)\n",
    "    return y\n",
    "\n",
    "\n",
    "def loss(pred_y, y):\n",
    "    cross_entropy = \\\n",
    "        tf.reduce_mean(-tf.reduce_sum(\n",
    "                       y * tf.log(tf.clip_by_value(pred_y, 1e-10, 1.0)),\n",
    "                       axis=1))\n",
    "    return cross_entropy\n",
    "\n",
    "\n",
    "def training(loss):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "    return train_step\n",
    "\n",
    "\n",
    "def accuracy(y, t):\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    1. データの準備\n",
    "    '''\n",
    "    mnist = datasets.fetch_mldata('MNIST original', data_home='.')\n",
    "\n",
    "    n = len(mnist.data)\n",
    "    N = 30000  # MNISTの一部を使う\n",
    "    N_train = 20000\n",
    "    N_validation = 4000\n",
    "    indices = np.random.permutation(range(n))[:N]  # ランダムにN枚を選択\n",
    "\n",
    "    X = mnist.data[indices]\n",
    "    y = mnist.target[indices]\n",
    "    Y = np.eye(10)[y.astype(int)]  # 1-of-K 表現に変換\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = \\\n",
    "        train_test_split(X, Y, train_size=N_train)\n",
    "\n",
    "    X_train, X_validation, Y_train, Y_validation = \\\n",
    "        train_test_split(X_train, Y_train, test_size=N_validation)\n",
    "\n",
    "    '''\n",
    "    2. モデル設定\n",
    "    '''\n",
    "    n_in = len(X[0])\n",
    "    n_hiddens = [200, 200, 200]  # 各隠れ層の次元数\n",
    "    n_out = len(Y[0])\n",
    "    p_keep = 0.5\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, n_in])\n",
    "    t = tf.placeholder(tf.float32, shape=[None, n_out])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    model = inference(x, keep_prob, n_in=n_in, n_hiddens=n_hiddens, n_out=n_out)\n",
    "    loss = loss(model, t)\n",
    "    train_step = training(loss)\n",
    "\n",
    "    accuracy = accuracy(model, t)\n",
    "\n",
    "    history = {\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "    '''\n",
    "    3. モデル学習\n",
    "    '''\n",
    "    epochs = 50\n",
    "    batch_size = 200\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    n_batches = N_train // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        X_, Y_ = shuffle(X_train, Y_train)\n",
    "\n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "\n",
    "            sess.run(train_step, feed_dict={\n",
    "                x: X_[start:end],\n",
    "                t: Y_[start:end],\n",
    "                keep_prob: p_keep\n",
    "            })\n",
    "\n",
    "        # 検証データを用いた評価\n",
    "        val_loss = loss.eval(session=sess, feed_dict={\n",
    "            x: X_validation,\n",
    "            t: Y_validation,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "        val_acc = accuracy.eval(session=sess, feed_dict={\n",
    "            x: X_validation,\n",
    "            t: Y_validation,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "\n",
    "        # 検証データに対する学習の進み具合を記録\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print('epoch:', epoch,\n",
    "              ' validation loss:', val_loss,\n",
    "              ' validation accuracy:', val_acc)\n",
    "        \n",
    "    '''\n",
    "    予測精度の評価\n",
    "    '''\n",
    "    accuracy_rate = accuracy.eval(session=sess, feed_dict={\n",
    "        x: X_test,\n",
    "        y: Y_test,\n",
    "        keep_prob: 1.0\n",
    "    })\n",
    "    print('accuracy: ', accuracy_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kerasによる実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/taiyou/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "16000/16000 [==============================] - 1s 92us/step - loss: 2.2604 - acc: 0.1547 - val_loss: 2.0656 - val_acc: 0.4352\n",
      "Epoch 2/50\n",
      "16000/16000 [==============================] - 1s 63us/step - loss: 1.4294 - acc: 0.5261 - val_loss: 0.7200 - val_acc: 0.7723\n",
      "Epoch 3/50\n",
      "16000/16000 [==============================] - 1s 73us/step - loss: 0.8072 - acc: 0.7357 - val_loss: 0.4901 - val_acc: 0.8595\n",
      "Epoch 4/50\n",
      "16000/16000 [==============================] - 1s 58us/step - loss: 0.6132 - acc: 0.8106 - val_loss: 0.3981 - val_acc: 0.8840\n",
      "Epoch 5/50\n",
      "16000/16000 [==============================] - 1s 65us/step - loss: 0.5120 - acc: 0.8460 - val_loss: 0.3326 - val_acc: 0.9048\n",
      "Epoch 6/50\n",
      "16000/16000 [==============================] - 1s 56us/step - loss: 0.4366 - acc: 0.8721 - val_loss: 0.3011 - val_acc: 0.9110\n",
      "Epoch 7/50\n",
      "16000/16000 [==============================] - 1s 49us/step - loss: 0.3898 - acc: 0.8868 - val_loss: 0.2590 - val_acc: 0.9235\n",
      "Epoch 8/50\n",
      "16000/16000 [==============================] - 1s 57us/step - loss: 0.3504 - acc: 0.8984 - val_loss: 0.2342 - val_acc: 0.9327\n",
      "Epoch 9/50\n",
      "16000/16000 [==============================] - 1s 53us/step - loss: 0.3235 - acc: 0.9074 - val_loss: 0.2215 - val_acc: 0.9352\n",
      "Epoch 10/50\n",
      "16000/16000 [==============================] - 1s 61us/step - loss: 0.2987 - acc: 0.9131 - val_loss: 0.2094 - val_acc: 0.9398\n",
      "Epoch 11/50\n",
      "16000/16000 [==============================] - 1s 64us/step - loss: 0.2800 - acc: 0.9201 - val_loss: 0.2045 - val_acc: 0.9420\n",
      "Epoch 12/50\n",
      "16000/16000 [==============================] - 1s 53us/step - loss: 0.2581 - acc: 0.9267 - val_loss: 0.1859 - val_acc: 0.9468\n",
      "Epoch 13/50\n",
      "16000/16000 [==============================] - 1s 63us/step - loss: 0.2463 - acc: 0.9297 - val_loss: 0.1883 - val_acc: 0.9458\n",
      "Epoch 14/50\n",
      "16000/16000 [==============================] - 1s 65us/step - loss: 0.2368 - acc: 0.9304 - val_loss: 0.1799 - val_acc: 0.9453\n",
      "Epoch 15/50\n",
      "16000/16000 [==============================] - 1s 64us/step - loss: 0.2158 - acc: 0.9373 - val_loss: 0.1725 - val_acc: 0.9500\n",
      "Epoch 16/50\n",
      "16000/16000 [==============================] - 1s 67us/step - loss: 0.2205 - acc: 0.9359 - val_loss: 0.1700 - val_acc: 0.9488\n",
      "Epoch 17/50\n",
      "16000/16000 [==============================] - 1s 69us/step - loss: 0.1985 - acc: 0.9438 - val_loss: 0.1658 - val_acc: 0.9525\n",
      "Epoch 18/50\n",
      "16000/16000 [==============================] - 1s 61us/step - loss: 0.1946 - acc: 0.9451 - val_loss: 0.1621 - val_acc: 0.9542\n",
      "Epoch 19/50\n",
      "16000/16000 [==============================] - 1s 61us/step - loss: 0.1839 - acc: 0.9462 - val_loss: 0.1605 - val_acc: 0.9553\n",
      "Epoch 20/50\n",
      "16000/16000 [==============================] - 1s 58us/step - loss: 0.1798 - acc: 0.9469 - val_loss: 0.1539 - val_acc: 0.9530\n",
      "Epoch 21/50\n",
      "16000/16000 [==============================] - 1s 59us/step - loss: 0.1791 - acc: 0.9501 - val_loss: 0.1529 - val_acc: 0.9588\n",
      "Epoch 22/50\n",
      "16000/16000 [==============================] - 1s 53us/step - loss: 0.1629 - acc: 0.9541 - val_loss: 0.1580 - val_acc: 0.9535\n",
      "Epoch 23/50\n",
      "16000/16000 [==============================] - 1s 49us/step - loss: 0.1630 - acc: 0.9519 - val_loss: 0.1590 - val_acc: 0.9570\n",
      "Epoch 24/50\n",
      "16000/16000 [==============================] - 1s 51us/step - loss: 0.1571 - acc: 0.9531 - val_loss: 0.1495 - val_acc: 0.9600\n",
      "Epoch 25/50\n",
      "16000/16000 [==============================] - 1s 54us/step - loss: 0.1408 - acc: 0.9605 - val_loss: 0.1481 - val_acc: 0.9598\n",
      "Epoch 26/50\n",
      "16000/16000 [==============================] - 1s 75us/step - loss: 0.1485 - acc: 0.9561 - val_loss: 0.1499 - val_acc: 0.9565\n",
      "Epoch 27/50\n",
      "16000/16000 [==============================] - 1s 89us/step - loss: 0.1441 - acc: 0.9599 - val_loss: 0.1515 - val_acc: 0.9567\n",
      "Epoch 28/50\n",
      "16000/16000 [==============================] - 1s 82us/step - loss: 0.1380 - acc: 0.9581 - val_loss: 0.1472 - val_acc: 0.9590\n",
      "Epoch 29/50\n",
      "16000/16000 [==============================] - 1s 87us/step - loss: 0.1290 - acc: 0.9617 - val_loss: 0.1524 - val_acc: 0.9587\n",
      "Epoch 30/50\n",
      "16000/16000 [==============================] - 1s 81us/step - loss: 0.1338 - acc: 0.9611 - val_loss: 0.1492 - val_acc: 0.9590\n",
      "Epoch 31/50\n",
      "16000/16000 [==============================] - 1s 72us/step - loss: 0.1292 - acc: 0.9615 - val_loss: 0.1510 - val_acc: 0.9578\n",
      "Epoch 32/50\n",
      "16000/16000 [==============================] - 1s 59us/step - loss: 0.1242 - acc: 0.9641 - val_loss: 0.1433 - val_acc: 0.9585\n",
      "Epoch 33/50\n",
      "16000/16000 [==============================] - 1s 49us/step - loss: 0.1259 - acc: 0.9610 - val_loss: 0.1525 - val_acc: 0.9605\n",
      "Epoch 34/50\n",
      "16000/16000 [==============================] - 1s 51us/step - loss: 0.1149 - acc: 0.9668 - val_loss: 0.1398 - val_acc: 0.9605\n",
      "Epoch 35/50\n",
      "16000/16000 [==============================] - 1s 48us/step - loss: 0.1177 - acc: 0.9649 - val_loss: 0.1387 - val_acc: 0.9615\n",
      "Epoch 36/50\n",
      "16000/16000 [==============================] - 1s 47us/step - loss: 0.1115 - acc: 0.9671 - val_loss: 0.1433 - val_acc: 0.9645\n",
      "Epoch 37/50\n",
      "16000/16000 [==============================] - 1s 49us/step - loss: 0.1111 - acc: 0.9681 - val_loss: 0.1459 - val_acc: 0.9597\n",
      "Epoch 38/50\n",
      "16000/16000 [==============================] - 1s 53us/step - loss: 0.1098 - acc: 0.9661 - val_loss: 0.1518 - val_acc: 0.9605\n",
      "Epoch 39/50\n",
      "16000/16000 [==============================] - 1s 50us/step - loss: 0.1095 - acc: 0.9665 - val_loss: 0.1398 - val_acc: 0.9635\n",
      "Epoch 40/50\n",
      "16000/16000 [==============================] - 1s 47us/step - loss: 0.1073 - acc: 0.9673 - val_loss: 0.1423 - val_acc: 0.9618\n",
      "Epoch 41/50\n",
      "16000/16000 [==============================] - 1s 47us/step - loss: 0.1050 - acc: 0.9692 - val_loss: 0.1494 - val_acc: 0.9603\n",
      "Epoch 42/50\n",
      "16000/16000 [==============================] - 1s 49us/step - loss: 0.1040 - acc: 0.9686 - val_loss: 0.1501 - val_acc: 0.9613\n",
      "Epoch 43/50\n",
      "16000/16000 [==============================] - 1s 48us/step - loss: 0.1009 - acc: 0.9694 - val_loss: 0.1559 - val_acc: 0.9600\n",
      "Epoch 44/50\n",
      "16000/16000 [==============================] - 1s 47us/step - loss: 0.0974 - acc: 0.9696 - val_loss: 0.1461 - val_acc: 0.9655\n",
      "Epoch 45/50\n",
      "16000/16000 [==============================] - 1s 44us/step - loss: 0.0929 - acc: 0.9728 - val_loss: 0.1541 - val_acc: 0.9638\n",
      "Epoch 46/50\n",
      "16000/16000 [==============================] - 1s 47us/step - loss: 0.0958 - acc: 0.9713 - val_loss: 0.1503 - val_acc: 0.9632\n",
      "Epoch 47/50\n",
      "16000/16000 [==============================] - 1s 50us/step - loss: 0.0871 - acc: 0.9746 - val_loss: 0.1509 - val_acc: 0.9637\n",
      "Epoch 48/50\n",
      "16000/16000 [==============================] - 1s 47us/step - loss: 0.0869 - acc: 0.9729 - val_loss: 0.1578 - val_acc: 0.9605\n",
      "Epoch 49/50\n",
      "16000/16000 [==============================] - 1s 44us/step - loss: 0.0843 - acc: 0.9724 - val_loss: 0.1431 - val_acc: 0.9653\n",
      "Epoch 50/50\n",
      "16000/16000 [==============================] - 1s 48us/step - loss: 0.0819 - acc: 0.9747 - val_loss: 0.1567 - val_acc: 0.9625\n",
      "10000/10000 [==============================] - 0s 33us/step\n",
      "[0.13862087070336565, 0.9656]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "'''\n",
    "1. データの生成\n",
    "'''\n",
    "mnist = datasets.fetch_mldata('MNIST original', data_home='.')\n",
    "\n",
    "n = len(mnist.data)\n",
    "N = 30000  # MNISTの一部を使う\n",
    "N_train = 20000\n",
    "N_validation = 4000\n",
    "indices = np.random.permutation(range(n))[:N]  # ランダムにN枚を選択\n",
    "\n",
    "X = mnist.data[indices]\n",
    "y = mnist.target[indices]\n",
    "Y = np.eye(10)[y.astype(int)]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = \\\n",
    "    train_test_split(X, Y, train_size=N_train)\n",
    "X_train, X_validation, Y_train, Y_validation = \\\n",
    "    train_test_split(X_train, Y_train, test_size=N_validation)\n",
    "\n",
    "'''\n",
    "2. モデル設定\n",
    "'''\n",
    "n_in = len(X[0])  # 784\n",
    "n_hiddens = [200, 200, 200]\n",
    "n_out = len(Y[0])  # 10\n",
    "p_keep = 0.5\n",
    "activation = 'relu'\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    return K.truncated_normal(shape, stddev=0.01)\n",
    "    # return np.random.normal(scale=0.01, size=shape)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "for i, input_dim in enumerate(([n_in] + n_hiddens)[:-1]):\n",
    "    model.add(Dense(n_hiddens[i], input_dim=input_dim,\n",
    "                    kernel_initializer=weight_variable))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(p_keep))\n",
    "\n",
    "model.add(Dense(n_out, kernel_initializer=weight_variable))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "'''\n",
    "3. モデル学習\n",
    "'''\n",
    "epochs = 50\n",
    "batch_size = 200\n",
    "\n",
    "hist = model.fit(X_train, Y_train, epochs=epochs,\n",
    "                 batch_size=batch_size,\n",
    "                 validation_data=(X_validation, Y_validation))\n",
    "\n",
    "\n",
    "'''\n",
    "4. 予測精度の評価\n",
    "'''\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlowにおけるモデルのクラス化\n",
    "Keras, sklearnのようなシンプルな記述をしたい。\n",
    "```python\n",
    "model = DNN()   # Deep Neural Net\n",
    "model.fit(X_train, Y_train)\n",
    "model.evaluate(X_test, Y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taiyou/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 2.2689435  accuracy: 0.0955\n",
      "epoch: 1  loss: 2.066787  accuracy: 0.32625\n",
      "epoch: 2  loss: 1.3807827  accuracy: 0.636\n",
      "epoch: 3  loss: 0.8205668  accuracy: 0.739625\n",
      "epoch: 4  loss: 0.60315645  accuracy: 0.84125\n",
      "epoch: 5  loss: 0.4939494  accuracy: 0.853875\n",
      "epoch: 6  loss: 0.41529793  accuracy: 0.8785\n",
      "epoch: 7  loss: 0.35941705  accuracy: 0.8955\n",
      "epoch: 8  loss: 0.31095868  accuracy: 0.909875\n",
      "epoch: 9  loss: 0.284324  accuracy: 0.919\n",
      "epoch: 10  loss: 0.2689737  accuracy: 0.92225\n",
      "epoch: 11  loss: 0.23972338  accuracy: 0.931625\n",
      "epoch: 12  loss: 0.22270766  accuracy: 0.934875\n",
      "epoch: 13  loss: 0.2149805  accuracy: 0.938\n",
      "epoch: 14  loss: 0.18783116  accuracy: 0.9455\n",
      "epoch: 15  loss: 0.17952396  accuracy: 0.947125\n",
      "epoch: 16  loss: 0.16210172  accuracy: 0.952625\n",
      "epoch: 17  loss: 0.15420085  accuracy: 0.95475\n",
      "epoch: 18  loss: 0.13554727  accuracy: 0.961125\n",
      "epoch: 19  loss: 0.13559642  accuracy: 0.958875\n",
      "epoch: 20  loss: 0.12772313  accuracy: 0.96275\n",
      "epoch: 21  loss: 0.11827733  accuracy: 0.965625\n",
      "epoch: 22  loss: 0.10608123  accuracy: 0.969875\n",
      "epoch: 23  loss: 0.09858642  accuracy: 0.971375\n",
      "epoch: 24  loss: 0.09622578  accuracy: 0.972\n",
      "epoch: 25  loss: 0.088088244  accuracy: 0.975625\n",
      "epoch: 26  loss: 0.083791934  accuracy: 0.977875\n",
      "epoch: 27  loss: 0.076028325  accuracy: 0.97975\n",
      "epoch: 28  loss: 0.071926944  accuracy: 0.980125\n",
      "epoch: 29  loss: 0.07484896  accuracy: 0.979\n",
      "accuracy:  0.944\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "\n",
    "class DNN(object):\n",
    "    def __init__(self, n_in, n_hiddens, n_out):\n",
    "        self.n_in = n_in\n",
    "        self.n_hiddens = n_hiddens\n",
    "        self.n_out = n_out\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        self._x = None\n",
    "        self._y = None\n",
    "        self._t = None,\n",
    "        self._keep_prob = None\n",
    "        self._sess = None\n",
    "        self._history = {\n",
    "            'accuracy': [],\n",
    "            'loss': []\n",
    "        }\n",
    "\n",
    "    def weight_variable(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.zeros(shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def inference(self, x, keep_prob):\n",
    "        \"\"\"\n",
    "        設定されたモデルを返す.\n",
    "        \"\"\"\n",
    "        # 入力層 - 隠れ層、隠れ層 - 隠れ層\n",
    "        for i, n_hidden in enumerate(self.n_hiddens):\n",
    "            if i == 0:\n",
    "                input = x\n",
    "                input_dim = self.n_in\n",
    "            else:\n",
    "                input = output\n",
    "                input_dim = self.n_hiddens[i-1]\n",
    "\n",
    "            self.weights.append(self.weight_variable([input_dim, n_hidden]))\n",
    "            self.biases.append(self.bias_variable([n_hidden]))\n",
    "\n",
    "            h = tf.nn.relu(tf.matmul(\n",
    "                input, self.weights[-1]) + self.biases[-1])\n",
    "            output = tf.nn.dropout(h, keep_prob)\n",
    "\n",
    "        # 隠れ層 - 出力層\n",
    "        self.weights.append(\n",
    "            self.weight_variable([self.n_hiddens[-1], self.n_out]))\n",
    "        self.biases.append(self.bias_variable([self.n_out]))\n",
    "\n",
    "        y = tf.nn.softmax(tf.matmul(\n",
    "            output, self.weights[-1]) + self.biases[-1])\n",
    "        return y\n",
    "\n",
    "    def loss(self, y, t):\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(t * tf.log(y), axis=1))\n",
    "        return cross_entropy\n",
    "\n",
    "    def training(self, loss):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "        train_step = optimizer.minimize(loss)\n",
    "        return train_step\n",
    "\n",
    "    def accuracy(self, y, t):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "\n",
    "    def fit(self, X_train, Y_train,\n",
    "            nb_epoch=100, batch_size=100, p_keep=0.5,\n",
    "            verbose=1):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.n_in])\n",
    "        t = tf.placeholder(tf.float32, shape=[None, self.n_out])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        self._x = x\n",
    "        self._t = t\n",
    "        self._keep_prob = keep_prob\n",
    "\n",
    "        y = self.inference(x, keep_prob)\n",
    "        loss = self.loss(y, t)\n",
    "        train_step = self.training(loss)\n",
    "        accuracy = self.accuracy(y, t)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "\n",
    "        self._y = y\n",
    "        self._sess = sess\n",
    "\n",
    "        N_train = len(X_train)\n",
    "        n_batches = N_train // batch_size\n",
    "\n",
    "        for epoch in range(nb_epoch):\n",
    "            X_, Y_ = shuffle(X_train, Y_train)\n",
    "\n",
    "            for i in range(n_batches):\n",
    "                start = i * batch_size\n",
    "                end = start + batch_size\n",
    "\n",
    "                sess.run(train_step, feed_dict={\n",
    "                    x: X_[start:end],\n",
    "                    t: Y_[start:end],\n",
    "                    keep_prob: p_keep\n",
    "                })\n",
    "            loss_ = loss.eval(session=sess, feed_dict={\n",
    "                x: X_train,\n",
    "                t: Y_train,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            accuracy_ = accuracy.eval(session=sess, feed_dict={\n",
    "                x: X_train,\n",
    "                t: Y_train,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            self._history['loss'].append(loss_)\n",
    "            self._history['accuracy'].append(accuracy_)\n",
    "\n",
    "            if verbose:\n",
    "                print('epoch:', epoch,\n",
    "                      ' loss:', loss_,\n",
    "                      ' accuracy:', accuracy_)\n",
    "\n",
    "        return self._history\n",
    "\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        accuracy = self.accuracy(self._y, self._t)\n",
    "        return accuracy.eval(session=self._sess, feed_dict={\n",
    "            self._x: X_test,\n",
    "            self._t: Y_test,\n",
    "            self._keep_prob: 1.0\n",
    "        })\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    データの生成\n",
    "    '''\n",
    "    mnist = datasets.fetch_mldata('MNIST original', data_home='.')\n",
    "\n",
    "    n = len(mnist.data)\n",
    "    N = 10000  # MNISTの一部を使う\n",
    "    indices = np.random.permutation(range(n))[:N]  # ランダムにN枚を選択\n",
    "\n",
    "    X = mnist.data[indices]\n",
    "    y = mnist.target[indices]\n",
    "    Y = np.eye(10)[y.astype(int)]  # 1-of-K 表現に変換\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8)\n",
    "\n",
    "    '''\n",
    "    モデル設定\n",
    "    '''\n",
    "    model = DNN(n_in=len(X[0]),\n",
    "                n_hiddens=[200, 200, 200],\n",
    "                n_out=len(Y[0]))\n",
    "\n",
    "    '''\n",
    "    モデル学習\n",
    "    '''\n",
    "    model.fit(X_train, Y_train,\n",
    "              nb_epoch=30,\n",
    "              batch_size=200,\n",
    "              p_keep=0.5)\n",
    "\n",
    "    '''\n",
    "    予測精度の評価\n",
    "    '''\n",
    "    accuracy = model.evaluate(X_test, Y_test)\n",
    "    print('accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
