# 7 再帰型ニューラルネット

## 7.1 系列データの分類

 - 系列データ : 個々の要素が順序付きの集まりとして与えられるデータ: $\mathrm {\boldsymbol {x}}^{1}, \mathrm {\boldsymbol {x}}^{2}, \mathrm {\boldsymbol {x}}^{3}, \dots, \mathrm {\boldsymbol {x}}^{T}$
 	- 系列の長さ$T$は一般的に可変
 - データの並びをインデックス$t = 1,2,3,\dots$で表し、以降$t$を便宜上、時刻と呼ぶ
 - 個々の要素が順序を持ち、しかもその並びに意味が隠れているようなデータ全般が対象となる

---
## 7.2 RNNの構造
> **再帰型ニューラルネット**(RNN) : 内部に(有向)閉路を持つニューラルネットの総称

この構造のおかげで、情報を一時的に記録し、また振る舞いを動的に変化させることが可能になる

| RNNの種類 | 説明 |
|:---------|:-----|
| Elmanネット | ... |
| Jordanネット | ... |
| 時間遅れネット(time delay-) | ... |
| エコー状態ネット(echo state) | ... |
| 単純再帰型ネット(Simple RNN) | 順伝播型ネットワークと同様の構造を持ち、ただし中間層のユニットの出力が自分自身に戻される「帰還路」をもつシンプルなRNN. |
| LSTM | Simple RNNでも理論上は上手くいきますが、現実的にはかなり前の古い情報を考慮するようには学習されません.他のDeep NeuralNetworksと同様に、勾配消失の問題に直面したからです. LSTMは、従来のRNNセルでは長期依存が必要なタスクを学習することができなかった問題を解決したモデルです. |
| GRU(Gated Recurrent Unit) | LSTMをもう少しシンプルにしたモデルです。入力ゲートと忘却ゲートを「更新ゲート」として１つのゲートに統合しています. |
| 双方向性RNN(bidirectional RNN) | 双方向性RNNは、過去の情報だけでなく、未来の情報を加味することで精度を向上させるためのモデルです. 一般的なRNNでは、過去から未来のみの情報で学習しますが、双方向性RNNは未来から過去の方向でも同時に学習します. |

ここでは、以下の図のように、単純再帰型ネットを考える

### 単純再帰型ネット(Simple RNN)
**モデル**

<img src="./imgs/07再帰型ニューラルネット/再帰型ニューラルネットの概要.png" width="80%">

 - ↑のRNNの動作 : 各時刻$t$につき1つの入力$\mathrm {\boldsymbol {x}}^{t}$を受け取り、また同時に1つの出力$\mathrm {\boldsymbol {y}}^{t}$を返す
 - ネットワーク内部にある帰還路によって、出力を計算する際、RNNが過去に受け取ったすべての入力(=入力の履歴)が関与する

<img src="./imgs/07再帰型ニューラルネット/RNNはデータ系列を受け取り出力の系列を生成する.png" width="60%">

**出力層**

 - 順伝播型ネットワーク同様に設計できる.
 	- ex) 分類問題 : ソフトマックス関数, シグモイド関数

**誤差関数**

 - 出力系列 : $\boldsymbol { \mathrm {y}}^{1}, \dots, \boldsymbol { \mathrm {y}}^{T}$
 - 目標となる系列$\boldsymbol { \mathrm {d}}^{1}, \dots, \boldsymbol { \mathrm {d}}^{T}$

目標系列と入力系列のペアからなる訓練データに対し、

$$
E\left( \boldsymbol {\mathrm {w}} \right) = \frac {1}{N} \sum _{n}{\sum _{ t }{ { \ell  }\left( d^{t}_{n},  f^{t}\left( \boldsymbol {\mathrm {x}}_{n} ; \boldsymbol {\mathrm {w}} \right) \right) } }
$$

とする。

 - ${\ell}\left(\cdot\right) \ge 0$: 個々の事例データに対して定義する誤差関数, 損失関数
 - $d^{t}_{n}$ : $n$番目のサンプルの時刻$t$での**目標出力**
 - $f^{t}\left( \boldsymbol {\mathrm {x}}_{n} \right)$ : 目標出力と比較されるRNNの**出力関数**
 - $\boldsymbol {\mathrm {x}}_{n}$ : $n$番目のサンプルの**入力系列**
 - 系列の長さはサンプル$n$ごとに違っていて構わない


---
## 7.3 順伝播計算
入力系列から出力系列を得る計算手順を考える.

 - ネットワークへの入力 : $\boldsymbol {\mathrm {x}}^{t} = \left( x_{i}^{t} \right)$
 - 中間層ユニットへの入力 : $\boldsymbol {\mathrm {u}}^{t} = \left( u_{j}^{t} \right)$
 - 中間層ユニットの出力 : $\boldsymbol {\mathrm {z}}^{t} = \left( z_{j}^{t} \right)$
 - 出力層ユニットへの入力 : $\boldsymbol {\mathrm {v}}^{t} = \left( v_{k}^{t} \right)$
 - 出力層ユニットの出力 : $\boldsymbol {\mathrm {y}}^{t} = \left( y_{k}^{t} \right)$
 - 目標出力 : $\boldsymbol {\mathrm {d}}^{t} = \left( d_{k}^{t} \right)$


RNNの帰還路は、中間層の出力を自らの入力に戻すが、この間の結合は全ユニット間で存在する。下の図のように時刻$t-1$中間層の任意のユニット$j^{\prime}$から時刻$t$中間層の任意のユニット$j$へ、重み$w_{j j^{\prime}}$の結合が存在する。

<div style="text-align: center;">
	重要なことは<b><font color="red">この帰還が、時刻を1つ隔てて行われること</font></b>
	<img src="./imgs/07再帰型ニューラルネット/順伝播計算.png" width="80%">
</div>

 - 入力層と中間層間の重み:  $\boldsymbol {\mathrm {W}}^{(in)} = \left( w^{(in)}_{ji} \right)$
 - 中間層から中間層への帰還路の結合の重み : $\boldsymbol {\mathrm {W}} = \left( w_{j{j}^{\prime}} \right)$
 - 中間層と出力層間の重み : $\boldsymbol {\mathrm {W}}^{(out)} = \left( w^{(out)}_{kj} \right)$
 - ※重みは時刻$t$とは関係なく、(学習によって更新はされるものの)順伝播計算中は定数であることに注意


**各ユニットへの入力**

上図から時刻$t$における中間層の各ユニットへの入力は、同時刻$t$にて入力層から届くものと、時刻$t-1$の中間層の出力をフィードバックしたものとの和になる。
$$
{ u }_{ j }^{ t }=\sum _{ i }{ { w }_{ ji }^{ (in) }{ x }_{ i }^{ t } } +\sum _{ { j }^{ \prime  } }{ { w }_{ j{ j }^{ \prime  } }{ z }_{ { j }^{ \prime  } }^{ t-1 } } 
$$


**各ユニットの出力**

通常の活性化関数$f$を経由して
$$
{ z }_{ j }^{ t }=f\left( { u }_{ j }^{ t } \right) 
$$
と計算される。

まとめると、
$$
\boldsymbol {\mathrm {z}}^{t} = \boldsymbol {\mathrm {f}}\left( \boldsymbol {\mathrm {W}}^{(in)} \boldsymbol {\mathrm {x}}^{t} + \boldsymbol {\mathrm {W}} \boldsymbol {\mathrm {z}}^{t-1} \right)
$$
となる。

 - $t=1$から始め、$t$を1つずつ増やしながら、入力系列$\mathrm {x}^{1}, \mathrm {x}^{2}, \dots$を使って、上式を繰り返し計算することで、任意の時刻$t$における中間層の状態$\boldsymbol {\mathrm {z}}^{t}$を求めることができる
 - ただし、$t=1$における初期値$\boldsymbol {\mathrm {z}}^{0} = \left( z_{j}^{0} \right)$を与える必要があり、通常は$z_{j}^{0}$とする


**出力層**

RNNの出力$\boldsymbol {\mathrm {y}}^{t}$は次のように計算する.まず出力層の各ユニットへの入力は、中間層の出力$\boldsymbol {\mathrm {z}}^{t}$から
$$
v^{t}_{k} = \sum _{j}{ w^{(out)}_{kj} z^{t}_{j} }
$$
と決まる.なお、出力層の活性化関数は、順伝播型ネットワーク同様、適用したい問題によって選ぶ。活性化関数を$\boldsymbol {\mathrm {f}}^{out}$と書くと、以上をまとめて
$$
\boldsymbol {\mathrm {y}}^{t} = \boldsymbol {\mathrm {f}}^{out}\left( \boldsymbol {\mathrm {v}}^{t} \right) = \boldsymbol {\mathrm {f}}^{out}\left( \boldsymbol {\mathrm {W}}^{(out)} \boldsymbol {\mathrm {z}}^{t} \right)
$$
と書くことができる


---
## 7.4 逆伝播計算
> RNNの学習には、順伝播型ネットワーク同様に確率的勾配降下法が使われる.

RNNの各層の重みについての誤差の微分を計算する方法

| 誤差微分の計算方法 | 説明 |
|:----------------|:--------|
| **RTRL法**(realtime recurrent learning) | メモリ効率がよい |
| **BPTT法**(backpropagation through time) | 計算速度が速い&シンプル |



### BPTT法

P.117~P.120「7.4 逆伝播計算」『機械学習プロフェッショナルシリーズ　深層学習』を参照


---
## 7.5 長・短期記憶(LSTM)
### RNNの勾配消失問題

 - 系列データの文脈を捉えて推定を行うため、現時刻からどれだけ遠い過去の入力を出力に反映させるかは重要です。
 - 順伝播型ネットワークの勾配消失/爆発問題からRNNは実際、高々過去10時刻分程度しか反映されていない
 - RNNでは、短期的な記憶は実現できても、より長期にわたる記憶を実現するのは難しい

### LSTMの概要
長期にわたる記憶を実現できるようにするモデル → **長・短期記憶**(**Long Short-Term Memory**; **LSTM**)

LSTMは、上で述べた基本的なRNNに対し、**<font color="blue">その中間層の各ユニットをメモリユニットと呼ぶ要素で置き換えた構造を持つ</font>**。入出力層などのそれ以外の構造は元のRNNとまったく変わらない。

メモリユニット1つの内部構造<br>
<div style="text-align: center">
	<img src="./imgs/07再帰型ニューラルネット/LSTMのメモリユニット.png">
	<div style="text-align: left;">
		大きな矢印は、外部からの入力を表し、これは入力層から届くものと、中間層(=全メモリユニット)の出力を帰還させたものを合わせたもの。ユニットb,c,d,fはすべてこの入力(に異なる重みを掛けたもの)を受け取る.ユニットeはメモリセルの出力に活性化関数を適用する.
	</div>
</div>

| 項目 | 説明 |
|:----:|:-----------|
| **メモリセル**(a) | 状態$s^{t}_{j}$を保持し、これを1時刻を隔ててメモリセル自身に帰還することで記憶を実現する |
| ユニットf | ユニット$f$の出力がゲートの値$g^{F,t}_{j} \in \left[0, 1\right]$となる. |
| **忘却ゲート** | メモリセルの帰還路には途中に忘却ゲートが挿入されおり、$s^{t}_{j}$に$g^{F,t}_{j}$を掛けたものが伝えられ、<br>$g^{F,t}_{j}$が1に近ければ現状態がそのまま記憶され、0に近ければリセット(忘却)される |
| ユニットb(入力) | メモリユニットへの外部からの入力はユニットbが受け取り、その出力がメモリセルに入力される.通常のRNNの中間層のユニット1つに相当する |
| ユニットc | 出力がゲート値$g^{I,t}_{j} \in \left[ 0, 1 \right]$になっている. |
| **入力ゲート** | ユニットb(入力)の出力に$g^{I,t}_{j}$を掛けたものがメモリセルに伝えられる |
| ユニットe | このユニットeを経てメモリユニットから外部へ出力される. |
| **出力ゲート** | ユニットdの出力がゲートの値$g^{O,t}_{j} \in \left[ 0, 1 \right]$になっている. この値が1に近ければメモリセルの出力は外部に伝達され、0に近ければブロックされる. |

> 以上の仕組みは、**短期間の記憶しか実現できないというRNNの限界を緩和することを狙ったもの**である.<br>
> 単純なケースでは忘却ゲートを1(オープン)、入力ゲートを0(クローズ)にし続けると、メモリセルの状態は永遠に記憶され続ける.

### 順伝播計算
上記のメモリユニットを式で書く.

$j$番目のメモリユニット内部のメモリセルは変数$s^{t}_{j}$を保持する.メモリセルの帰還路は変数$s^{t}_{j}$の中身を1時刻分将来に引き継ぎます.
$$
s^{t}_{j} = g^{F,t}_{j} \cdot s^{t-1}_{j} + g^{I,t}_{j} \cdot f\left( u^{t}_{j} \right)
$$

メモリユニット$j$が受け取る入力は、元のRNN同様、入力層と前の時刻の中間層から次のように入力を受け取る.

$$
u^{t}_{j} = \sum _{i}{w^{in}_{ji} x^{t}_{i}} + \sum _{j^{\prime}}{w_{j {j}^{\prime}} z^{t-1}_{j^{\prime}}}
$$

忘却ゲートの値$g^{F,t}_{j}$, 入力ゲートの値$g^{I,t}_{j}$
$$
g^{F,t}_{j} = f\left( u^{F,t}_{j} \right) = f\left( \sum _{i}{ w^{F,in}_{ji} x^{t}_{i}} + \sum _{j^{\prime}}{ w^{I}_{j{j}^{\prime}} z^{t-1}_{j^{\prime}} } + w^{F}_{j} s^{t-1}_{j} \right)
$$

$$
g^{I,t}_{j} = f\left( u^{I,t}_{j} \right) = f\left( \sum _{i}{ w^{I,in}_{ji} x^{t}_{i}} + \sum _{j^{\prime}}{ w^{I}_{j{j}^{\prime}} z^{t-1}_{j^{\prime}} } + w^{I}_{j} s^{t-1}_{j} \right)
$$

 - $w^{F}_{j}, w^{I}_{j}$ : メモリセルから忘却ゲートと入力ゲートの値を決めるユニットへの結合の$s^{t-1}_{j}$の重み
 	- これらは、下で扱う出力ゲートに関する同様の結合と合わせて、「**<font color="blue">のぞき穴</font>(peephole)**」結合とも呼ばれている


メモリユニットからの出力
$$
z^{t}_{j} = g^{O,t}_{j} \cdot f\left( s^{t}_{j} \right)
$$

ただし、$g^{O,t}_{j}$は出力ゲートの値
$$
g^{O,t}_{j} = f\left( u^{O,t}_{j} \right) = f\left( \sum _{i}{ w^{O,in}_{ji} x^{t}_{i}} + \sum _{j^{\prime}}{ w^{O}_{j{j}^{\prime}} z^{t-1}_{j^{\prime}} } + w^{O}_{j} s^{t}_{j} \right)
$$

 - 出力ゲートのみ、**<font color="red">$s^{t-1}_{j}$ではなく$s^{t}_{j}$を加算する</font>**ことに注意すること！
 - 各ゲートの値$g^{F,t}_{j}, g^{I,t}_{j}, g^{O,t}_{j}$は、その計算に用いる活性化関数$f$をロジスティックシグモイド関数とすることで、値域を$\left[0,1\right]$に制約する.
 - メモリユニットの出力$z^{t}_{j}$は、次の時刻の3種のゲート(入力ゲート,忘却ゲート,出力ゲート)を制御するユニットへの入力となる他,出力層のユニットへの入力にもなり、さらに出力層の活性化関数に従って時刻$t$のネットワークの出力$\boldsymbol {\mathrm {y}}^{t}$が確定する

### 逆順伝播計算

P.124~P.125「7.5.4 逆伝播計算」『機械学習プロフェッショナルシリーズ　深層学習』を参照


---
## 7.6 入出力間で系列長が異なる場合
### 隠れマルコフモデル



### コネクショニスト時系列分類法





